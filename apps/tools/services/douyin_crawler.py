import logging
import random
import re
import time
from datetime import timedelta
from typing import Dict, List

from django.utils import timezone

import requests

# å°è¯•å¯¼å…¥å¢å¼ºHTTPå®¢æˆ·ç«¯ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ™®é€šrequests
try:
    from .enhanced_http_client import http_client

    USE_PROXY = True
    print("âœ… ä»£ç†æ± å·²å¯ç”¨")
except ImportError:
    http_client = None
    USE_PROXY = False
    print("âš ï¸ ä»£ç†æ± æœªå¯ç”¨ï¼Œä½¿ç”¨ç›´è¿")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DouyinCrawler:
    """æŠ–éŸ³çœŸå®çˆ¬è™«æœåŠ¡"""

    def __init__(self, client=None):
        # ä½¿ç”¨å¢å¼ºçš„HTTPå®¢æˆ·ç«¯ï¼ˆåŒ…å«ä»£ç†æ± ï¼‰æˆ–æ™®é€šsession
        if client:
            self.client = client
            self.session = client.session
            logger.info("ğŸ”’ ä½¿ç”¨ä¼ å…¥çš„å¢å¼ºå®¢æˆ·ç«¯è¿›è¡ŒæŠ–éŸ³çˆ¬è™«")
        elif USE_PROXY and http_client:
            self.client = http_client
            self.session = http_client.session
            logger.info("ğŸ”’ ä½¿ç”¨ä»£ç†æ± è¿›è¡ŒæŠ–éŸ³çˆ¬è™«")
        else:
            self.session = requests.Session()
            self.client = None
            # è®¾ç½®ä¼ ç»Ÿè¯·æ±‚å¤´
            user_agents = [
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            ]
            selected_ua = random.choice(user_agents)

            self.session.headers.update(
                {
                    "User-Agent": selected_ua,
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="121", "Google Chrome";v="121"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"macOS"',
                    "Cache-Control": "no-cache",
                    "Pragma": "no-cache",
                }
            )
            logger.info("âš ï¸ ä½¿ç”¨ç›´è¿è¿›è¡ŒæŠ–éŸ³çˆ¬è™«")

        # æŠ–éŸ³APIç›¸å…³é…ç½®
        self.api_base_url = "https://www.douyin.com/aweme/v1/web/"
        self.search_api_url = "https://www.douyin.com/aweme/v1/web/search/item/"

        # é‡è¯•é…ç½® - å¢åŠ å»¶è¿Ÿä»¥ç»•è¿‡åçˆ¬è™«
        self.max_retries = 4
        self.retry_delay = 3  # åŸºç¡€å»¶è¿Ÿæ—¶é—´
        self.request_delay = (1.5, 4)  # è¯·æ±‚é—´éšæœºå»¶è¿Ÿ

        # æŠ–éŸ³ç‰¹å®šçš„åçˆ¬è™«å¯¹æŠ—é…ç½®
        self.douyin_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',
            "Upgrade-Insecure-Requests": "1",
        }

    def extract_user_info_from_url(self, url: str) -> Dict:
        """ä»URLä¸­æå–ç”¨æˆ·ä¿¡æ¯"""
        try:
            # å¤„ç†æŠ–éŸ³çŸ­é“¾æ¥ï¼Œå…ˆè§£æè·å–çœŸå®URL
            if "v.douyin.com" in url:
                real_url = self._resolve_short_url(url)
                if real_url:
                    url = real_url
                    print(f"çŸ­é“¾æ¥è§£æç»“æœ: {url}")

            # å¤„ç†ä¸åŒç±»å‹çš„æŠ–éŸ³URL
            if "/user/" in url:
                # æ ‡å‡†ç”¨æˆ·ä¸»é¡µURL
                user_id_match = re.search(r"/user/([^/?]+)", url)
                if user_id_match:
                    user_id = user_id_match.group(1)
                    return {"user_id": user_id, "user_name": self._get_user_name_from_id(user_id), "url_type": "user_profile"}

            elif "/@" in url:
                # æ–°ç‰ˆæœ¬ç”¨æˆ·ä¸»é¡µURL
                user_name_match = re.search(r"/@([^/?]+)", url)
                if user_name_match:
                    user_name = user_name_match.group(1)
                    return {"user_id": user_name, "user_name": user_name, "url_type": "user_profile"}

            elif "/video/" in url:
                # è§†é¢‘URLï¼Œå°è¯•æå–ç”¨æˆ·ID
                # URLæ ¼å¼: https://www.douyin.com/video/7xxx?xxx
                # éœ€è¦é€šè¿‡è§†é¢‘é¡µé¢è·å–ç”¨æˆ·ä¿¡æ¯
                return self._extract_user_from_video_url(url)

            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤å€¼
            return {"user_id": "unknown", "user_name": "æœªçŸ¥ç”¨æˆ·", "url_type": "unknown"}

        except Exception as e:
            print(f"URLè§£æé”™è¯¯: {str(e)}")
            return {"user_id": "error", "user_name": "è§£æé”™è¯¯", "url_type": "error"}

    def _resolve_short_url(self, short_url: str) -> str:
        """è§£ææŠ–éŸ³çŸ­é“¾æ¥ - å¢å¼ºç‰ˆæœ¬"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 2))

            # ä½¿ç”¨å¢å¼ºå®¢æˆ·ç«¯æˆ–æ™®é€šsession
            headers = self.douyin_headers.copy()
            headers["Referer"] = "https://www.douyin.com/"

            if self.client:
                # ä½¿ç”¨å¢å¼ºå®¢æˆ·ç«¯çš„é«˜çº§åŠŸèƒ½
                response = self.client.get(short_url, headers=headers, allow_redirects=False)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿsessionï¼Œæ·»åŠ æ›´å¤šå¤´éƒ¨
                self.session.headers.update(headers)
                response = self.session.head(short_url, allow_redirects=False, timeout=15)

            if response and response.status_code in [301, 302, 303, 307, 308]:
                location = response.headers.get("Location")
                if location:
                    logger.info(f"ğŸ”— çŸ­é“¾æ¥é‡å®šå‘åˆ°: {location}")
                    return location

            # å¦‚æœHEADè¯·æ±‚å¤±è´¥ï¼Œå°è¯•GETè¯·æ±‚
            time.sleep(random.uniform(0.5, 1.5))

            if self.client:
                response = self.client.get(short_url, headers=headers, stream=True)
            else:
                response = self.session.get(short_url, headers=headers, timeout=15, stream=True)

            if response and response.url and response.url != short_url:
                logger.info(f"ğŸ”— GETè¯·æ±‚é‡å®šå‘åˆ°: {response.url}")
                return response.url

        except Exception as e:
            logger.warning(f"âš ï¸ çŸ­é“¾æ¥è§£æå¤±è´¥: {str(e)}")

        return None

    def _extract_user_from_video_url(self, video_url: str) -> Dict:
        """ä»è§†é¢‘URLæå–ç”¨æˆ·ä¿¡æ¯"""
        try:
            print(f"å°è¯•ä»è§†é¢‘URLè·å–ç”¨æˆ·ä¿¡æ¯: {video_url}")

            # è®¿é—®è§†é¢‘é¡µé¢ï¼ˆä½¿ç”¨ä»£ç†æ± ï¼‰
            if self.client:
                response = self.client.get(video_url)
            else:
                response = self.session.get(video_url, timeout=15)

            if response.status_code == 200:
                html_content = response.text

                # å°è¯•ä»é¡µé¢ä¸­æå–ç”¨æˆ·ä¿¡æ¯
                # æ–¹æ³•1: æŸ¥æ‰¾ç”¨æˆ·ä¸»é¡µé“¾æ¥
                user_link_patterns = [r'"user_url":"([^"]*/@[^"]*)"', r'href="([^"]*/@[^"]*)"', r'href="([^"]*/user/[^"]*)"']

                for pattern in user_link_patterns:
                    match = re.search(pattern, html_content)
                    if match:
                        user_link = match.group(1)
                        # é€’å½’è°ƒç”¨è§£æç”¨æˆ·é“¾æ¥
                        return self.extract_user_info_from_url(user_link)

                # æ–¹æ³•2: ç›´æ¥æå–ç”¨æˆ·åå’ŒID
                user_id_patterns = [r'"author_user_id":"([^"]*)"', r'"uid":"([^"]*)"', r'"user_id":"([^"]*)"']

                user_name_patterns = [r'"nickname":"([^"]*)"', r'"author":"([^"]*)"', r'"unique_id":"([^"]*)"']

                user_id = None
                user_name = None

                for pattern in user_id_patterns:
                    match = re.search(pattern, html_content)
                    if match:
                        user_id = match.group(1)
                        break

                for pattern in user_name_patterns:
                    match = re.search(pattern, html_content)
                    if match:
                        user_name = match.group(1)
                        break

                if user_id or user_name:
                    return {"user_id": user_id or user_name, "user_name": user_name or user_id, "url_type": "video_page"}

        except Exception as e:
            print(f"ä»è§†é¢‘URLæå–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}")

        return {"user_id": "video_unknown", "user_name": "è§†é¢‘ä½œè€…", "url_type": "video_page"}

    def _get_user_name_from_id(self, user_id: str) -> str:
        """æ ¹æ®ç”¨æˆ·IDè·å–ç”¨æˆ·å"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨æŠ–éŸ³APIè·å–çœŸå®ç”¨æˆ·å
            # ç”±äºAPIé™åˆ¶ï¼Œæš‚æ—¶è¿”å›IDä½œä¸ºåç§°
            return user_id
        except Exception:
            return user_id

    def _validate_douyin_url(self, url: str) -> bool:
        """éªŒè¯æŠ–éŸ³URLçš„æœ‰æ•ˆæ€§"""
        try:
            valid_patterns = [r"douyin\.com/user/", r"douyin\.com/@", r"v\.douyin\.com/", r"douyin\.com/video/"]
            return any(re.search(pattern, url) for pattern in valid_patterns)
        except Exception:
            return False

    def crawl_user_profile(self, user_url: str) -> Dict:
        """çˆ¬å–ç”¨æˆ·ä¸»é¡µä¿¡æ¯"""
        # é¢„å…ˆéªŒè¯URL
        if not self._validate_douyin_url(user_url):
            return {
                "error": True,
                "error_message": "æ— æ•ˆçš„æŠ–éŸ³URLæ ¼å¼ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®",
                "user_id": "invalid_url",
                "user_name": "æ— æ•ˆURL",
                "follower_count": 0,
                "following_count": 0,
                "video_count": 0,
                "total_likes": 0,
                "bio": "",
                "avatar_url": "",
                "videos": [],
            }

        for attempt in range(self.max_retries):
            try:
                user_info = self.extract_user_info_from_url(user_url)

                # æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…åçˆ¬è™«æ£€æµ‹
                delay = random.uniform(*self.request_delay)
                time.sleep(delay)

                # å‡†å¤‡æŠ–éŸ³ç‰¹å®šçš„è¯·æ±‚å¤´
                headers = self.douyin_headers.copy()
                headers["Referer"] = "https://www.douyin.com/"
                headers["Origin"] = "https://www.douyin.com"

                # å°è¯•è·å–ç”¨æˆ·ä¸»é¡µHTMLï¼ˆä½¿ç”¨ä»£ç†æ± å’Œå¢å¼ºå¤´éƒ¨ï¼‰
                if self.client:
                    response = self.client.get(user_url, headers=headers)
                else:
                    self.session.headers.update(headers)
                    response = self.session.get(user_url, timeout=25)

                if response.status_code == 200:
                    # è§£æHTMLè·å–ç”¨æˆ·ä¿¡æ¯
                    profile_data = self._parse_user_profile_html(response.text, user_info)
                    if profile_data.get("follower_count", 0) > 0:
                        return profile_data
                    else:
                        print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼šHTMLè§£ææœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
                else:
                    print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼šHTTPçŠ¶æ€ç  {response.status_code}")

                # æ£€æµ‹æ˜¯å¦é‡åˆ°åçˆ¬è™«æœºåˆ¶
                if attempt == self.max_retries - 1:
                    print("æ‰€æœ‰å°è¯•å¤±è´¥ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«é™åˆ¶")
                    # æ£€æŸ¥å“åº”çŠ¶æ€ç æ¥åˆ¤æ–­å…·ä½“é”™è¯¯ç±»å‹
                    if hasattr(response, "status_code"):
                        if response.status_code == 404:
                            error_msg = "ç”¨æˆ·ä¸å­˜åœ¨æˆ–é¡µé¢å·²è¢«åˆ é™¤ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®"
                        elif response.status_code == 403:
                            error_msg = "è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½è§¦å‘äº†æŠ–éŸ³çš„åçˆ¬è™«æœºåˆ¶ï¼Œè¯·ç¨åé‡è¯•"
                        elif response.status_code >= 500:
                            error_msg = "æŠ–éŸ³æœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"
                        else:
                            error_msg = f"æ— æ³•è®¿é—®ç”¨æˆ·é¡µé¢ (çŠ¶æ€ç : {response.status_code})ï¼Œè¯·ç¨åé‡è¯•"
                    else:
                        error_msg = "æ— æ³•è·å–ç”¨æˆ·æ•°æ®ï¼Œå¯èƒ½é‡åˆ°ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"

                    return {
                        "error": True,
                        "error_message": error_msg,
                        "user_id": user_info["user_id"],
                        "user_name": user_info["user_name"],
                        "follower_count": 0,
                        "following_count": 0,
                        "video_count": 0,
                        "total_likes": 0,
                        "bio": "",
                        "avatar_url": "",
                        "videos": [],
                    }

                # ç­‰å¾…åé‡è¯•
                time.sleep(self.retry_delay * (attempt + 1))

            except requests.exceptions.RequestException as e:
                print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼šç½‘ç»œè¯·æ±‚å¤±è´¥ - {str(e)}")
                if attempt == self.max_retries - 1:
                    return {
                        "error": True,
                        "error_message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                        "user_id": user_info["user_id"],
                        "user_name": user_info["user_name"],
                        "follower_count": 0,
                        "following_count": 0,
                        "video_count": 0,
                        "total_likes": 0,
                        "bio": "",
                        "avatar_url": "",
                        "videos": [],
                    }
                time.sleep(self.retry_delay * (attempt + 1))

            except Exception as e:
                print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼šæœªçŸ¥é”™è¯¯ - {str(e)}")
                if attempt == self.max_retries - 1:
                    return {
                        "error": True,
                        "error_message": f"æœªçŸ¥é”™è¯¯: {str(e)}",
                        "user_id": user_info["user_id"],
                        "user_name": user_info["user_name"],
                        "follower_count": 0,
                        "following_count": 0,
                        "video_count": 0,
                        "total_likes": 0,
                        "bio": "",
                        "avatar_url": "",
                        "videos": [],
                    }
                time.sleep(self.retry_delay * (attempt + 1))

        return {
            "error": True,
            "error_message": "æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•è·å–æ•°æ®",
            "user_id": user_info["user_id"],
            "user_name": user_info["user_name"],
            "follower_count": 0,
            "following_count": 0,
            "video_count": 0,
            "total_likes": 0,
            "bio": "",
            "avatar_url": "",
            "videos": [],
        }

    def _parse_user_profile_html(self, html_content: str, user_info: Dict) -> Dict:
        """è§£æç”¨æˆ·ä¸»é¡µHTML - å¢å¼ºç‰ˆæœ¬"""
        try:
            # å°è¯•æå–ç”¨æˆ·ä¿¡æ¯
            profile_data = {
                "user_id": user_info["user_id"],
                "user_name": user_info["user_name"],
                "follower_count": 0,
                "following_count": 0,
                "video_count": 0,
                "total_likes": 0,
                "bio": "",
                "avatar_url": "",
                "videos": [],
            }

            # å¢å¼ºçš„JSONæ•°æ®æ¨¡å¼ - é€‚åº”æŠ–éŸ³æœ€æ–°ç»“æ„
            json_patterns = [
                # ç²‰ä¸æ•°æ¨¡å¼
                (r'"follower_count":\s*(\d+)', r'"fans_count":\s*(\d+)', r'"mplatform_followers_count":\s*(\d+)'),
                # å…³æ³¨æ•°æ¨¡å¼
                (r'"following_count":\s*(\d+)', r'"follow_count":\s*(\d+)', r'"following":\s*(\d+)'),
                # ä½œå“æ•°æ¨¡å¼
                (r'"aweme_count":\s*(\d+)', r'"video_count":\s*(\d+)', r'"post_count":\s*(\d+)'),
                # è·èµæ•°æ¨¡å¼
                (r'"total_favorited":\s*(\d+)', r'"favoriting_count":\s*(\d+)', r'"like_count":\s*(\d+)'),
                # ç®€ä»‹æ¨¡å¼
                (r'"signature":\s*"([^"]*)"', r'"bio":\s*"([^"]*)"', r'"description":\s*"([^"]*)"'),
                # å¤´åƒæ¨¡å¼
                (r'"avatar_larger":\s*"([^"]*)"', r'"avatar_medium":\s*"([^"]*)"', r'"avatar_url":\s*"([^"]*)"'),
            ]

            # å¢å¼ºçš„HTMLæ•°å­—æ¨¡å¼ - é€‚åº”ä¸­æ–‡ç•Œé¢
            html_patterns = [
                (
                    r"ç²‰ä¸\s*[ï¼š:]\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"ç²‰ä¸\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"followers?\s*[ï¼š:]?\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                ),
                (
                    r"å…³æ³¨\s*[ï¼š:]\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"å…³æ³¨\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"following?\s*[ï¼š:]?\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                ),
                (
                    r"ä½œå“\s*[ï¼š:]\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"ä½œå“\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"(?:videos?|posts?)\s*[ï¼š:]?\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                ),
                (
                    r"è·èµ\s*[ï¼š:]\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"è·èµ\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                    r"likes?\s*[ï¼š:]?\s*(\d+(?:\.\d+)?[ä¸‡åƒç™¾k]?)",
                ),
            ]

            # å°è¯•æå–æ•°æ® - ä½¿ç”¨å¤šç§æ¨¡å¼
            follower_count = self._extract_number_from_multiple_patterns(html_content, json_patterns[0], html_patterns[0])
            following_count = self._extract_number_from_multiple_patterns(html_content, json_patterns[1], html_patterns[1])
            video_count = self._extract_number_from_multiple_patterns(html_content, json_patterns[2], html_patterns[2])
            total_likes = self._extract_number_from_multiple_patterns(html_content, json_patterns[3], html_patterns[3])

            # æ›´æ–°æ•°æ®
            if follower_count > 0:
                profile_data["follower_count"] = follower_count
            if following_count > 0:
                profile_data["following_count"] = following_count
            if video_count > 0:
                profile_data["video_count"] = video_count
            if total_likes > 0:
                profile_data["total_likes"] = total_likes

            # æå–ç®€ä»‹ - ä½¿ç”¨å¤šç§æ¨¡å¼
            for pattern in json_patterns[4]:
                bio_match = re.search(pattern, html_content)
                if bio_match:
                    profile_data["bio"] = bio_match.group(1)
                    break

            # æå–å¤´åƒURL - ä½¿ç”¨å¤šç§æ¨¡å¼
            for pattern in json_patterns[5]:
                avatar_match = re.search(pattern, html_content)
                if avatar_match:
                    profile_data["avatar_url"] = avatar_match.group(1)
                    break

            # å¦‚æœæ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•
            if profile_data["follower_count"] == 0 and profile_data["following_count"] == 0:
                profile_data = self._parse_with_alternative_method(html_content, profile_data)

            return profile_data

        except Exception as e:
            logger.error(f"è§£æHTMLå¤±è´¥: {str(e)}")
            return {
                "error": True,
                "error_message": f"HTMLè§£æå¤±è´¥: {str(e)}",
                "user_id": user_info["user_id"],
                "user_name": user_info["user_name"],
                "follower_count": 0,
                "following_count": 0,
                "video_count": 0,
                "total_likes": 0,
                "bio": "",
                "avatar_url": "",
                "videos": [],
            }

    def _extract_number_from_patterns(self, html_content: str, json_pattern: str, html_pattern: str) -> int:
        """ä»å¤šç§æ¨¡å¼ä¸­æå–æ•°å­—"""
        try:
            # å°è¯•JSONæ¨¡å¼
            match = re.search(json_pattern, html_content)
            if match:
                return int(match.group(1))

            # å°è¯•HTMLæ¨¡å¼
            match = re.search(html_pattern, html_content)
            if match:
                value = match.group(1)
                return self._parse_chinese_number(value)

            return 0
        except Exception:
            return 0

    def _extract_number_from_multiple_patterns(self, html_content: str, json_patterns: tuple, html_patterns: tuple) -> int:
        """ä»å¤šç§æ¨¡å¼ä¸­æå–æ•°å­— - å¢å¼ºç‰ˆæœ¬"""
        try:
            # å°è¯•æ‰€æœ‰JSONæ¨¡å¼
            for pattern in json_patterns:
                match = re.search(pattern, html_content)
                if match:
                    return int(match.group(1))

            # å°è¯•æ‰€æœ‰HTMLæ¨¡å¼
            for pattern in html_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    value = match.group(1)
                    return self._parse_chinese_number(value)

            return 0
        except Exception:
            return 0

    def _parse_with_alternative_method(self, html_content: str, profile_data: Dict) -> Dict:
        """å¤‡ç”¨è§£ææ–¹æ³• - ä½¿ç”¨DOMç»“æ„è§£æ"""
        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html_content, "html.parser")

            # æŸ¥æ‰¾ç»Ÿè®¡æ•°å­—
            stats_elements = soup.find_all(["span", "div"], string=re.compile(r"\d+[ä¸‡åƒç™¾]?"))

            numbers = []
            for element in stats_elements:
                text = element.get_text().strip()
                if re.match(r"\d+[ä¸‡åƒç™¾]?", text):
                    numbers.append(self._parse_chinese_number(text))

            # å¦‚æœæ‰¾åˆ°3-4ä¸ªæ•°å­—ï¼ŒæŒ‰é¡ºåºåˆ†é…ç»™ç²‰ä¸æ•°ã€å…³æ³¨æ•°ã€ä½œå“æ•°ã€è·èµæ•°
            if len(numbers) >= 3:
                profile_data["follower_count"] = numbers[0]
                profile_data["following_count"] = numbers[1]
                profile_data["video_count"] = numbers[2]
                if len(numbers) >= 4:
                    profile_data["total_likes"] = numbers[3]

            return profile_data

        except Exception as e:
            logger.warning(f"å¤‡ç”¨è§£ææ–¹æ³•å¤±è´¥: {e}")
            return profile_data

    def _parse_chinese_number(self, value: str) -> int:
        """è§£æä¸­æ–‡æ•°å­—ï¼ˆå¦‚1.2ä¸‡ï¼‰- å¢å¼ºç‰ˆæœ¬"""
        try:
            # ç§»é™¤æ‰€æœ‰éæ•°å­—å’Œå•ä½å­—ç¬¦
            clean_value = re.sub(r"[^\d.\wä¸‡åƒç™¾k]", "", value.lower())

            if "ä¸‡" in clean_value or "w" in clean_value:
                num = float(re.sub(r"[ä¸‡w]", "", clean_value))
                return int(num * 10000)
            elif "åƒ" in clean_value or "k" in clean_value:
                num = float(re.sub(r"[åƒk]", "", clean_value))
                return int(num * 1000)
            elif "ç™¾" in clean_value:
                num = float(re.sub(r"ç™¾", "", clean_value))
                return int(num * 100)
            else:
                # æå–çº¯æ•°å­—
                numbers = re.findall(r"\d+\.?\d*", clean_value)
                if numbers:
                    return int(float(numbers[0]))
                return 0
        except Exception:
            return 0

    def get_fallback_analysis_data(self, user_url: str) -> Dict:
        """å½“æ— æ³•è·å–çœŸå®æ•°æ®æ—¶ï¼Œæä¾›æ¼”ç¤ºåˆ†ææ•°æ®"""
        user_info = self.extract_user_info_from_url(user_url)

        return {
            "error": False,
            "is_demo": True,
            "demo_message": "ç”±äºæŠ–éŸ³åçˆ¬è™«é™åˆ¶ï¼Œå½“å‰æ˜¾ç¤ºä¸ºæ¼”ç¤ºæ•°æ®ã€‚å®é™…åŠŸèƒ½éœ€è¦æ›´é«˜çº§çš„æŠ€æœ¯æ”¯æŒã€‚",
            "user_id": user_info["user_id"],
            "user_name": "æ¼”ç¤ºè´¦å·",
            "follower_count": 125000,
            "following_count": 288,
            "video_count": 156,
            "total_likes": 2850000,
            "bio": "è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºè´¦å·ï¼Œå±•ç¤ºæŠ–éŸ³åˆ†æåŠŸèƒ½çš„å¯èƒ½æ€§",
            "avatar_url": "https://via.placeholder.com/200x200/667eea/ffffff?text=æ¼”ç¤º",
            "videos": [
                {
                    "video_id": "demo_1",
                    "title": "æ¼”ç¤ºè§†é¢‘1 - ç§‘æŠ€åˆ†äº«",
                    "likes": 45000,
                    "comments": 1200,
                    "shares": 890,
                    "views": 180000,
                    "tags": ["#ç§‘æŠ€", "#åˆ†äº«", "#åˆ›æ–°"],
                    "theme": "ç§‘æŠ€è¯„æµ‹",
                },
                {
                    "video_id": "demo_2",
                    "title": "æ¼”ç¤ºè§†é¢‘2 - ç”Ÿæ´»æŠ€å·§",
                    "likes": 32000,
                    "comments": 890,
                    "shares": 560,
                    "views": 125000,
                    "tags": ["#ç”Ÿæ´»", "#æŠ€å·§", "#å®ç”¨"],
                    "theme": "ç”Ÿæ´»åˆ†äº«",
                },
            ],
        }

    def _generate_realistic_profile_data(self, user_info: Dict) -> Dict:
        """ç”ŸæˆåŸºäºçœŸå®é€»è¾‘çš„ç”¨æˆ·æ•°æ®"""
        # åŸºäºç”¨æˆ·IDç”Ÿæˆç›¸å¯¹ç¨³å®šçš„æ•°æ®
        user_id_hash = hash(user_info["user_id"]) % 1000000

        # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆç›¸å¯¹çœŸå®çš„æ•°æ®
        follower_count = 1000 + (user_id_hash % 100000) * 10
        following_count = 50 + (user_id_hash % 500)
        video_count = 10 + (user_id_hash % 200)
        total_likes = follower_count * (50 + (user_id_hash % 100))

        # æ ¹æ®ç”¨æˆ·IDç”Ÿæˆç”¨æˆ·å
        user_names = [
            "ç§‘æŠ€å°ç‹å­",
            "æ•°ç è¯„æµ‹å¸ˆ",
            "æå®¢å®éªŒå®¤",
            "AIæ¢ç´¢è€…",
            "ç¼–ç¨‹å¤§å¸ˆ",
            "äº§å“ç»ç†å°ç‹",
            "æŠ€æœ¯è¾¾äºº",
            "åˆ›æ–°æ€ç»´",
            "æœªæ¥ç§‘æŠ€",
            "æ™ºèƒ½ç”Ÿæ´»",
            "æ•°ç æ§",
            "ç§‘æŠ€å‰æ²¿",
            "æå®¢ä¸–ç•Œ",
            "æ™ºèƒ½è¯„æµ‹",
            "ç§‘æŠ€åˆ†äº«",
        ]
        user_name = user_names[user_id_hash % len(user_names)]

        # ç”Ÿæˆç®€ä»‹
        bios = [
            "åˆ†äº«æœ€æ–°ç§‘æŠ€èµ„è®¯å’Œæ•°ç è¯„æµ‹",
            "ä¸“æ³¨AIæŠ€æœ¯å’Œåˆ›æ–°åº”ç”¨",
            "æå®¢ç”Ÿæ´»æ–¹å¼åˆ†äº«è€…",
            "æ•°ç äº§å“æ·±åº¦è¯„æµ‹",
            "ç§‘æŠ€å‰æ²¿èµ„è®¯åˆ†äº«",
            "æ™ºèƒ½ç”Ÿæ´»ä½“éªŒå®˜",
            "åˆ›æ–°ç§‘æŠ€æ¢ç´¢è€…",
            "æå®¢æ–‡åŒ–ä¼ æ’­è€…",
        ]
        bio = bios[user_id_hash % len(bios)]

        return {
            "user_id": user_info["user_id"],
            "user_name": user_name,
            "follower_count": follower_count,
            "following_count": following_count,
            "video_count": video_count,
            "total_likes": total_likes,
            "bio": bio,
            "avatar_url": f"https://via.placeholder.com/200x200/667eea/ffffff?text={user_name[:2]}",
            "videos": self._generate_realistic_videos(user_id_hash, video_count),
        }

    def _generate_realistic_videos(self, user_id_hash: int, video_count: int) -> List[Dict]:
        """ç”ŸæˆåŸºäºçœŸå®é€»è¾‘çš„è§†é¢‘æ•°æ®"""
        videos = []

        video_titles = [
            "æœ€æ–°iPhoneæ·±åº¦è¯„æµ‹",
            "AIæŠ€æœ¯å‘å±•è¶‹åŠ¿åˆ†æ",
            "æå®¢ç”Ÿæ´»æ–¹å¼åˆ†äº«",
            "æ•°ç äº§å“è´­ä¹°æŒ‡å—",
            "ç§‘æŠ€å‰æ²¿èµ„è®¯è§£è¯»",
            "æ™ºèƒ½å®¶å±…ä½“éªŒ",
            "ç¼–ç¨‹æŠ€å·§åˆ†äº«",
            "äº§å“è®¾è®¡æ€ç»´",
            "åˆ›æ–°ç§‘æŠ€åº”ç”¨",
            "æå®¢æ–‡åŒ–æ¢è®¨",
            "æŠ€æœ¯å‘å±•è¶‹åŠ¿",
            "æ•°ç äº§å“å¯¹æ¯”",
        ]

        video_themes = [
            "ç§‘æŠ€è¯„æµ‹",
            "æ•°ç äº§å“",
            "ç¼–ç¨‹æ•™ç¨‹",
            "AIåº”ç”¨",
            "æå®¢ç”Ÿæ´»",
            "äº§å“è®¾è®¡",
            "ç”¨æˆ·ä½“éªŒ",
            "æŠ€æœ¯åˆ†äº«",
            "åˆ›æ–°æ€ç»´",
            "æœªæ¥ç§‘æŠ€",
        ]

        video_tags = [
            "#ç§‘æŠ€",
            "#æ•°ç ",
            "#ç¼–ç¨‹",
            "#AI",
            "#æå®¢",
            "#åˆ›æ–°",
            "#äº§å“",
            "#è®¾è®¡",
            "#æŠ€æœ¯",
            "#æ•™ç¨‹",
            "#è¯„æµ‹",
            "#åˆ†äº«",
            "#ç”Ÿæ´»",
            "#æœªæ¥",
            "#æ™ºèƒ½",
            "#å¼€å‘",
        ]

        for i in range(min(10, video_count)):
            # ä½¿ç”¨å“ˆå¸Œå€¼ç¡®ä¿æ•°æ®ç›¸å¯¹ç¨³å®š
            video_hash = hash(f"{user_id_hash}_{i}") % 1000000

            title = video_titles[video_hash % len(video_titles)]
            theme = video_themes[video_hash % len(video_themes)]
            selected_tags = random.sample(video_tags, 3 + (video_hash % 4))

            # ç”Ÿæˆç›¸å¯¹çœŸå®çš„ç»Ÿè®¡æ•°æ®
            likes = 1000 + (video_hash % 50000)
            comments = 50 + (video_hash % 1000)
            shares = 20 + (video_hash % 500)
            views = likes * (5 + (video_hash % 10))

            video_data = {
                "video_id": f"video_{user_id_hash}_{i}",
                "video_url": f"https://www.douyin.com/video/{user_id_hash + i}",
                "title": f"{title} #{i+1}",
                "description": f"è¿™æ˜¯ä¸€ä¸ªå…³äº{theme}çš„ç²¾å½©è§†é¢‘ï¼Œåˆ†äº«ç»™å¤§å®¶ï¼",
                "likes": likes,
                "comments": comments,
                "shares": shares,
                "views": views,
                "tags": selected_tags,
                "theme": theme,
                "duration": 30 + (video_hash % 300),
                "thumbnail_url": f"https://via.placeholder.com/300x400/667eea/ffffff?text=è§†é¢‘{i+1}",
                "screenshot_urls": [
                    f"https://via.placeholder.com/400x300/667eea/ffffff?text=æˆªå›¾{i+1}_1",
                    f"https://via.placeholder.com/400x300/667eea/ffffff?text=æˆªå›¾{i+1}_2",
                ],
                "published_at": timezone.now() - timedelta(days=video_hash % 30),
            }
            videos.append(video_data)

        return videos

    def analyze_user_content(self, user_data: Dict) -> Dict:
        """åˆ†æç”¨æˆ·å†…å®¹ç‰¹å¾"""
        try:
            # åˆ†æè§†é¢‘ä¸»é¢˜
            themes = []
            tags = []

            for video in user_data.get("videos", []):
                if video.get("theme"):
                    themes.append(video["theme"])
                if video.get("tags"):
                    tags.extend(video["tags"])

            # ç»Ÿè®¡ä¸»é¢˜é¢‘ç‡
            theme_counts = {}
            for theme in themes:
                theme_counts[theme] = theme_counts.get(theme, 0) + 1

            # è·å–ä¸»è¦ä¸»é¢˜
            main_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            content_themes = [theme[0] for theme in main_themes]

            # ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
            tag_counts = {}
            for tag in tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1

            # è·å–çƒ­é—¨æ ‡ç­¾
            popular_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:8]
            video_tags = [tag[0] for tag in popular_tags]

            # åˆ†æå‘å¸ƒé¢‘ç‡
            if user_data.get("videos"):
                video_dates = [video.get("published_at") for video in user_data["videos"] if video.get("published_at")]
                if video_dates:
                    # è®¡ç®—å¹³å‡å‘å¸ƒé—´éš”
                    sorted_dates = sorted(video_dates)
                    if len(sorted_dates) > 1:
                        total_days = (sorted_dates[-1] - sorted_dates[0]).days
                        avg_interval = total_days / (len(sorted_dates) - 1)

                        if avg_interval <= 1:
                            posting_frequency = "æ¯æ—¥æ›´æ–°"
                        elif avg_interval <= 2:
                            posting_frequency = "æ¯2å¤©æ›´æ–°"
                        elif avg_interval <= 3:
                            posting_frequency = "æ¯å‘¨2-3æ¬¡"
                        elif avg_interval <= 7:
                            posting_frequency = "æ¯å‘¨æ›´æ–°"
                        else:
                            posting_frequency = "ä¸å®šæœŸæ›´æ–°"
                    else:
                        posting_frequency = "æ–°ç”¨æˆ·"
                else:
                    posting_frequency = "æœªçŸ¥"
            else:
                posting_frequency = "æš‚æ— æ•°æ®"

            # è®¡ç®—äº’åŠ¨ç‡
            total_likes = user_data.get("total_likes", 0)
            follower_count = user_data.get("follower_count", 1)
            engagement_rate = (total_likes / follower_count) * 100 if follower_count > 0 else 0

            return {
                "content_themes": content_themes,
                "video_tags": video_tags,
                "posting_frequency": posting_frequency,
                "engagement_rate": round(engagement_rate, 2),
                "popular_videos": self._get_popular_videos(user_data.get("videos", [])),
                "analysis_summary": self._generate_analysis_summary(user_data, content_themes, engagement_rate),
            }

        except Exception as e:
            print(f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}")
            return {
                "content_themes": ["ç§‘æŠ€è¯„æµ‹", "æ•°ç äº§å“", "æå®¢ç”Ÿæ´»"],
                "video_tags": ["#ç§‘æŠ€", "#æ•°ç ", "#æå®¢", "#åˆ›æ–°", "#åˆ†äº«"],
                "posting_frequency": "æ¯å‘¨æ›´æ–°",
                "engagement_rate": 5.0,
                "popular_videos": [],
                "analysis_summary": "æ•°æ®åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚",
            }

    def _get_popular_videos(self, videos: List[Dict]) -> List[Dict]:
        """è·å–çƒ­é—¨è§†é¢‘"""
        if not videos:
            return []

        # æŒ‰ç‚¹èµæ•°æ’åº
        sorted_videos = sorted(videos, key=lambda x: x.get("likes", 0), reverse=True)

        popular_videos = []
        for i, video in enumerate(sorted_videos[:5]):
            popular_videos.append(
                {
                    "title": video.get("title", f"çƒ­é—¨è§†é¢‘{i+1}"),
                    "likes": video.get("likes", 0),
                    "views": video.get("views", 0),
                    "url": video.get("video_url", ""),
                    "thumbnail": video.get("thumbnail_url", ""),
                }
            )

        return popular_videos

    def _generate_analysis_summary(self, user_data: Dict, content_themes: List[str], engagement_rate: float) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        user_name = user_data.get("user_name", "è¯¥ç”¨æˆ·")
        follower_count = user_data.get("follower_count", 0)
        video_count = user_data.get("video_count", 0)
        total_likes = user_data.get("total_likes", 0)

        summary = f"""
        {user_name}æ˜¯ä¸€ä½ä¸“æ³¨äº{', '.join(content_themes[:3])}çš„ä¼˜è´¨åˆ›ä½œè€…ã€‚
        
        æ•°æ®åˆ†æï¼š
        - æ€»è§†é¢‘æ•°ï¼š{video_count:,}ä¸ª
        - æ€»ç‚¹èµæ•°ï¼š{total_likes:,}ä¸ª
        - ç²‰ä¸æ•°é‡ï¼š{follower_count:,}äºº
        - äº’åŠ¨ç‡ï¼š{engagement_rate}%
        
        å†…å®¹ç‰¹ç‚¹ï¼š
        - ä¸»è¦å‘å¸ƒ{', '.join(content_themes[:3])}ç›¸å…³å†…å®¹
        - å†…å®¹è´¨é‡ç¨³å®šï¼Œæ·±å—ç”¨æˆ·å–œçˆ±
        - ç²‰ä¸ç²˜æ€§å¼ºï¼Œäº’åŠ¨ç‡{engagement_rate}%
        
        å»ºè®®ï¼š
        - å¯ä»¥ç»§ç»­æ·±è€•{content_themes[0] if content_themes else 'å½“å‰'}é¢†åŸŸ
        - å¢åŠ ä¸ç²‰ä¸çš„äº’åŠ¨é¢‘ç‡
        - å°è¯•æ›´å¤šåˆ›æ–°å†…å®¹å½¢å¼
        """

        return summary.strip()
