#!/usr/bin/env python3
"""
å¢å¼ºçš„HTTPå®¢æˆ·ç«¯ - é›†æˆä»£ç†æ± å’Œåçˆ¬è™«æœºåˆ¶
"""

import logging
import random
import time
from typing import Dict, Optional

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from .proxy_pool import proxy_pool

logger = logging.getLogger(__name__)


class EnhancedHTTPClient:
    """å¢å¼ºçš„HTTPå®¢æˆ·ç«¯ï¼Œæ”¯æŒä»£ç†è½®æ¢å’Œåçˆ¬è™«"""

    def __init__(self, use_proxy: bool = True, max_retries: int = 3):
        self.use_proxy = use_proxy
        self.max_retries = max_retries
        self.session = None
        self.current_proxy = None
        self.request_count = 0
        self.proxy_rotation_interval = 10  # æ¯10ä¸ªè¯·æ±‚è½®æ¢ä»£ç†

        self._init_session()

    def _init_session(self):
        """åˆå§‹åŒ–ä¼šè¯"""
        self.session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=self.max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1,
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        self._update_headers()

    def _update_headers(self):
        """æ›´æ–°è¯·æ±‚å¤´"""
        self.session.headers.update(
            {
                "User-Agent": proxy_pool.get_random_user_agent(),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
        )

    def _get_proxy(self) -> Optional[Dict]:
        """è·å–ä»£ç†é…ç½®"""
        if not self.use_proxy:
            return None

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢ä»£ç†
        if self.current_proxy is None or self.request_count % self.proxy_rotation_interval == 0:

            new_proxy = proxy_pool.get_proxy_for_requests()
            if new_proxy:
                self.current_proxy = new_proxy
                logger.info(f"ğŸ”„ åˆ‡æ¢ä»£ç†: {new_proxy['info'].proxy}")
            else:
                logger.warning("âš ï¸ æ— å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿")
                self.current_proxy = None

        return self.current_proxy

    def _add_random_delay(self):
        """æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«æ£€æµ‹"""
        delay = random.uniform(0.5, 2.5)  # 0.5-2.5ç§’éšæœºå»¶è¿Ÿï¼Œæ›´è´´è¿‘çœŸå®ç”¨æˆ·è¡Œä¸º
        time.sleep(delay)

    def _get_random_headers(self) -> Dict[str, str]:
        """è·å–éšæœºåŒ–çš„è¯·æ±‚å¤´"""
        base_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": random.choice(
                ["zh-CN,zh;q=0.9,en;q=0.8", "en-US,en;q=0.9,zh-CN;q=0.8", "zh-CN,zh;q=0.8,en-US;q=0.7,en;q=0.5"]
            ),
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": random.choice(["document", "empty", "image"]),
            "Sec-Fetch-Mode": random.choice(["navigate", "cors", "no-cors"]),
            "Sec-Fetch-Site": random.choice(["none", "same-origin", "cross-site"]),
            "Sec-Fetch-User": "?1",
            "Cache-Control": random.choice(["no-cache", "max-age=0", "no-store"]),
            "Pragma": "no-cache",
        }

        # éšæœºæ·»åŠ ä¸€äº›å¯é€‰å¤´éƒ¨
        optional_headers = {
            "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": f'"{random.choice(["Windows", "macOS", "Linux"])}"',
            "X-Requested-With": "XMLHttpRequest" if random.random() < 0.3 else None,
            "Referer": f'https://www.{random.choice(["google", "bing", "baidu"])}.com/' if random.random() < 0.4 else None,
        }

        for key, value in optional_headers.items():
            if value and random.random() < 0.7:  # 70%æ¦‚ç‡æ·»åŠ å¯é€‰å¤´éƒ¨
                base_headers[key] = value

        return base_headers

    def get(self, url: str, **kwargs) -> Optional[requests.Response]:
        """å¢å¼ºçš„GETè¯·æ±‚"""
        return self._make_request("GET", url, **kwargs)

    def post(self, url: str, **kwargs) -> Optional[requests.Response]:
        """å¢å¼ºçš„POSTè¯·æ±‚"""
        return self._make_request("POST", url, **kwargs)

    def _make_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """æ‰§è¡ŒHTTPè¯·æ±‚ï¼ŒåŒ…å«ä»£ç†fallbackæœºåˆ¶"""
        self.request_count += 1

        # æ·»åŠ éšæœºå»¶è¿Ÿ
        self._add_random_delay()

        # æ›´æ–°è¯·æ±‚å¤´ï¼ˆæ¯æ¬¡è¯·æ±‚éšæœºåŒ–ï¼‰
        if "headers" not in kwargs:
            kwargs["headers"] = {}

        # åˆå¹¶éšæœºå¤´éƒ¨
        random_headers = self._get_random_headers()
        for key, value in random_headers.items():
            if key not in kwargs["headers"]:
                kwargs["headers"][key] = value

        # ç¡®ä¿User-Agentæ˜¯éšæœºçš„
        kwargs["headers"]["User-Agent"] = proxy_pool.get_random_user_agent()

        # è®¾ç½®è¶…æ—¶
        if "timeout" not in kwargs:
            kwargs["timeout"] = 20

        # é¦–å…ˆå°è¯•ä½¿ç”¨ä»£ç†
        if self.use_proxy:
            proxy_config = self._get_proxy()
            if proxy_config:
                kwargs_with_proxy = kwargs.copy()
                kwargs_with_proxy["proxies"] = {"http": proxy_config["http"], "https": proxy_config["https"]}

                # å°è¯•ä½¿ç”¨ä»£ç†è¯·æ±‚
                response = self._try_request_with_config(method, url, proxy_config, **kwargs_with_proxy)
                if response:
                    return response

                logger.warning("ğŸ”„ ä»£ç†è¯·æ±‚å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼")

        # ä»£ç†å¤±è´¥æˆ–ä¸ä½¿ç”¨ä»£ç†æ—¶ï¼Œå°è¯•ç›´è¿
        return self._try_direct_request(method, url, **kwargs)

    def _try_request_with_config(self, method: str, url: str, proxy_config: Dict, **kwargs) -> Optional[requests.Response]:
        """ä½¿ç”¨æŒ‡å®šé…ç½®å°è¯•è¯·æ±‚"""
        for attempt in range(2):  # ä»£ç†æ¨¡å¼ä¸‹å‡å°‘é‡è¯•æ¬¡æ•°
            try:
                logger.debug(f"ğŸ”’ {method} {url} via proxy {proxy_config['info'].proxy} (å°è¯• {attempt + 1}/2)")

                if method.upper() == "GET":
                    response = self.session.get(url, **kwargs)
                elif method.upper() == "POST":
                    response = self.session.post(url, **kwargs)
                else:
                    response = self.session.request(method, url, **kwargs)

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    logger.debug(f"âœ… ä»£ç†è¯·æ±‚æˆåŠŸ: {url}")
                    return response
                elif response.status_code == 404:
                    logger.warning(f"âŒ é¡µé¢ä¸å­˜åœ¨ ({response.status_code}): {url}")
                    return response  # 404æ˜¯æœ‰æ•ˆå“åº”ï¼Œä¸é‡è¯•
                elif response.status_code == 403:
                    logger.warning(f"ğŸš« ä»£ç†è¢«æ‹’ç» ({response.status_code}): {url}")
                    # æ ‡è®°ä»£ç†å¤±è´¥
                    proxy_pool.mark_proxy_failed(proxy_config["info"].proxy)
                    self.current_proxy = None
                    break  # è·³å‡ºå¾ªç¯ï¼Œå°è¯•ç›´è¿
                else:
                    logger.warning(f"âš ï¸ ä»£ç†è¿”å›å¼‚å¸¸çŠ¶æ€ç  ({response.status_code}): {url}")

            except (requests.exceptions.ProxyError, requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                logger.warning(f"ğŸ”Œ ä»£ç†è¿æ¥é—®é¢˜: {e}")
                # æ ‡è®°ä»£ç†å¤±è´¥
                proxy_pool.mark_proxy_failed(proxy_config["info"].proxy)
                self.current_proxy = None
                break  # è·³å‡ºå¾ªç¯ï¼Œå°è¯•ç›´è¿

            except Exception as e:
                logger.warning(f"âš ï¸ ä»£ç†è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt == 1:  # æœ€åä¸€æ¬¡å°è¯•
                    break

            # çŸ­æš‚ç­‰å¾…åé‡è¯•
            if attempt < 1:
                time.sleep(1)

        return None

    def _try_direct_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """å°è¯•ç›´è¿è¯·æ±‚"""
        # ç§»é™¤ä»£ç†é…ç½®
        kwargs.pop("proxies", None)

        for attempt in range(self.max_retries):
            try:
                logger.debug(f"ğŸŒ {method} {url} via direct (å°è¯• {attempt + 1}/{self.max_retries})")

                if method.upper() == "GET":
                    response = self.session.get(url, **kwargs)
                elif method.upper() == "POST":
                    response = self.session.post(url, **kwargs)
                else:
                    response = self.session.request(method, url, **kwargs)

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    logger.debug(f"âœ… ç›´è¿è¯·æ±‚æˆåŠŸ: {url}")
                    return response
                elif response.status_code == 404:
                    logger.warning(f"âŒ é¡µé¢ä¸å­˜åœ¨ ({response.status_code}): {url}")
                    return response  # 404æ˜¯æœ‰æ•ˆå“åº”ï¼Œä¸é‡è¯•
                elif response.status_code == 403:
                    logger.warning(f"ğŸš« ç›´è¿ä¹Ÿè¢«æ‹’ç» ({response.status_code}): {url}")
                    # 403å¯èƒ½æ˜¯IPè¢«å°ï¼Œç¨ç­‰é‡è¯•
                    if attempt < self.max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        logger.info(f"â±ï¸ IPå¯èƒ½è¢«é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                        time.sleep(wait_time)
                else:
                    logger.warning(f"âš ï¸ ç›´è¿å¼‚å¸¸çŠ¶æ€ç  ({response.status_code}): {url}")

            except requests.exceptions.Timeout as e:
                logger.warning(f"â° ç›´è¿è¶…æ—¶: {e}")

            except requests.exceptions.ConnectionError as e:
                logger.warning(f"ğŸ”Œ ç›´è¿è¿æ¥é”™è¯¯: {e}")

            except Exception as e:
                logger.warning(f"âš ï¸ ç›´è¿è¯·æ±‚å¼‚å¸¸: {e}")

            # ç­‰å¾…åé‡è¯•
            if attempt < self.max_retries - 1:
                wait_time = (attempt + 1) * 2
                logger.info(f"â±ï¸ ç­‰å¾… {wait_time} ç§’åé‡è¯•ç›´è¿...")
                time.sleep(wait_time)

        logger.error(f"âŒ æ‰€æœ‰è¯·æ±‚æ–¹å¼éƒ½å¤±è´¥: {url}")
        return None

    def get_stats(self) -> Dict:
        """è·å–å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_requests": self.request_count,
            "use_proxy": self.use_proxy,
            "current_proxy": self.current_proxy["info"].proxy if self.current_proxy else None,
            "proxy_pool_stats": proxy_pool.get_stats(),
        }


# åˆ›å»ºå…¨å±€å®¢æˆ·ç«¯å®ä¾‹
http_client = EnhancedHTTPClient(use_proxy=True)
